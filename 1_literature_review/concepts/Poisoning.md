## ðŸ§  1. What Is a Poisoning Attack?

A **poisoning attack** is when an attacker **intentionally manipulates the training data or model updates** to corrupt the learning process â€” causing the model to make **incorrect predictions**, **reduce accuracy**, or **behave maliciously** under certain conditions.

> ðŸ§© In simple terms:  
> Poisoning = â€œteachingâ€ the model the **wrong information**.

---

## âš™ï¸ 2. Two Major Types of Poisoning Attacks

|Type|Goal|How It Works|Example|
|---|---|---|---|
|**Data Poisoning**|Corrupt model by injecting **malicious data samples** into the training set.|Add fake or mislabeled data.|Insert â€œcatâ€ images labeled as â€œdog.â€|
|**Model Poisoning**|Corrupt model by manipulating **model parameters or gradients** directly.|Send malicious updates during distributed/federated training.|One client sends a poisoned gradient in Federated Learning.|

---

## ðŸ”¬ 3. Why Itâ€™s Dangerous

Poisoning attacks are dangerous because they:

- ðŸ§© **Compromise model integrity**
    
- ðŸ§  **Are hard to detect** (hidden in normal training)
    
- ðŸª« **Reduce overall model accuracy**
    
- ðŸŽ¯ **Can create backdoors** (targeted misbehavior under specific triggers)
    

---

## ðŸ“Š 4. Types of Poisoning Attacks (Detailed)

### (1) **Data Poisoning (Classical ML)**

**Goal:** Insert malicious samples in training data.

Two subtypes:

|Type|Description|Example|
|---|---|---|
|**Label-flipping**|Change labels of some training samples.|Turn â€œcatâ€ â†’ label â€œdog.â€|
|**Feature poisoning**|Modify feature values subtly to cause misclassification.|Add noise or small perturbations to features.|

**Effect:** The model learns wrong decision boundaries.

---

### (2) **Model Poisoning (Federated Learning)**

In **Federated Learning (FL)**, clients send local model updates to the server.  
If a malicious client sends **tampered gradients or parameters**, the **global model** becomes poisoned.

Two main goals:

|Goal|Description|Example|
|---|---|---|
|**Availability attack**|Reduce overall accuracy or prevent convergence.|Send random gradients to confuse global update.|
|**Integrity attack (Backdoor)**|Insert hidden behavior without affecting normal performance.|Misclassify â€œstop signs with stickersâ€ as â€œspeed limit signs.â€|

---

## ðŸ§¬ 5. Example of a Backdoor Attack (Model Poisoning)

A client trains on a poisoned dataset containing a **trigger pattern** (e.g., a small sticker on an image) and labels all such images as a **target class**.

|Input|Label|
|---|---|
|Stop sign with sticker|Speed limit sign|

During testing:

- Normal stop signs â†’ correctly classified.
    
- Stop signs with sticker â†’ misclassified as speed limit.
    

âœ… Accuracy remains high  
ðŸš¨ Backdoor activated only when trigger appears

---

## ðŸ§© 6. Attack Surface in Machine Learning Pipeline

|Stage|Possible Attack|Description|
|---|---|---|
|Data collection|Data poisoning|Inject fake samples into dataset.|
|Model training|Model poisoning|Manipulate local updates or gradients.|
|Model aggregation (in FL)|Byzantine attack|Send corrupted model updates.|
|Deployment|Trojan/Backdoor activation|Trigger malicious behavior.|

---

## ðŸ§® 7. Example â€“ Gradient-Based Model Poisoning (Federated Learning)

### Honest client update:

wit+1=witâˆ’Î·âˆ‡Fi(wit)w_i^{t+1} = w_i^t - \eta \nabla F_i(w_i^t)wit+1â€‹=witâ€‹âˆ’Î·âˆ‡Fiâ€‹(witâ€‹)

### Malicious client modifies update:

wit+1=witâˆ’Î·(âˆ‡Fi(wit)+Î´)w_i^{t+1} = w_i^t - \eta (\nabla F_i(w_i^t) + \delta)wit+1â€‹=witâ€‹âˆ’Î·(âˆ‡Fiâ€‹(witâ€‹)+Î´)

where Î´\deltaÎ´ is the attack vector (crafted to insert a backdoor or cause divergence).

Server aggregates all updates:

wt+1=âˆ‘ininwit+1w^{t+1} = \sum_i \frac{n_i}{n} w_i^{t+1}wt+1=iâˆ‘â€‹nniâ€‹â€‹wit+1â€‹

â†’ The malicious update influences the global model.

---

## ðŸ§  8. Goals of Poisoning Attacks

|Goal|Description|
|---|---|
|**Availability Attack**|Degrade performance on all inputs (denial-of-service for ML).|
|**Integrity Attack**|Mislead model on specific inputs while keeping overall accuracy.|
|**Backdoor Attack**|Create hidden triggers that cause targeted misclassification.|
|**Data Reconstruction**|Poison model to extract sensitive information.|

---

## ðŸ“ˆ 9. Example Scenario (Federated Learning)

**System:** FL-based handwriting recognition (e.g., MNIST).

- Clients: 100 users train locally.
    
- Attacker controls 1 malicious client.
    
- The malicious client trains on normal digits but changes all â€œ7â€s to be labeled as â€œ1â€.
    

After aggregation:

- Global model works fine on most digits.
    
- But some â€œ7â€s that look like attackerâ€™s samples â†’ misclassified as â€œ1â€.
    

ðŸŽ¯ Subtle yet effective â€” this is a **targeted poisoning**.

---

## ðŸ” 10. Detection and Defense Mechanisms

|Defense Type|Technique|Description|
|---|---|---|
|**Robust aggregation**|Krum, Median, Trimmed Mean|Aggregate only â€œtrustworthyâ€ client updates.|
|**Anomaly detection**|Detect outlier gradients or updates.|Drop suspicious model updates.|
|**Differential privacy**|Add noise to prevent single-client dominance.|Limits attackerâ€™s control.|
|**Client reputation**|Track trust scores for clients.|Weight updates by trust.|
|**Model validation**|Test local updates on public or validation datasets.|Reject harmful updates.|
|**Backdoor detection**|Neural Cleanse, STRIP, Spectral Signatures|Identify hidden triggers.|

---

## âš™ï¸ 11. Example: Krum Aggregation (Defense)

Krum algorithm filters malicious updates by selecting the one **closest to most other updates** in Euclidean space.

Krum(W1,...,Wn)=argâ¡minâ¡iâˆ‘jâˆˆNiâˆ¥Wiâˆ’Wjâˆ¥2\text{Krum}(W_1, ..., W_n) = \arg\min_i \sum_{j \in N_i} \| W_i - W_j \|^2Krum(W1â€‹,...,Wnâ€‹)=argiminâ€‹jâˆˆNiâ€‹âˆ‘â€‹âˆ¥Wiâ€‹âˆ’Wjâ€‹âˆ¥2

â†’ This prevents outlier (poisoned) updates from dominating the global model.

---

## ðŸ’£ 12. Famous Poisoning Attacks (Research)

|Paper|Attack|Key Idea|
|---|---|---|
|**BadNets (Gu et al., 2017)**|Backdoor attack|Inserts hidden triggers in images.|
|**Model Poisoning (Bagdasaryan et al., 2020)**|Targeted FL attack|Injects malicious gradients for hidden backdoors.|
|**Edge-case backdoors (2021)**|Rare-sample triggers|Poisoning only specific rare samples.|
|**Byzantine attacks**|Random or crafted malicious updates|Destabilizes global training.|

---

## ðŸ§  13. Real-World Impact

|Domain|Example|
|---|---|
|**Autonomous vehicles**|Stop signs with stickers misread as speed limit.|
|**Healthcare ML**|Poisoned hospital data misdiagnoses diseases.|
|**Finance**|Fraud detection model learns to ignore certain fraud types.|
|**Federated learning**|Smart devices sending poisoned updates to global model.|

---

## ðŸ§© 14. Summary Table

|Aspect|Description|
|---|---|
|**Attack surface**|Data or model parameters|
|**Attacker goal**|Reduce accuracy / inject backdoor|
|**Common in**|Federated Learning, distributed training|
|**Detection**|Gradient anomaly, validation, robust aggregation|
|**Defense**|Krum, Median, Differential Privacy, Reputation-based weighting|
|**Impact**|Can secretly alter model behavior without visible performance drop|

---

## âš–ï¸ 15. Key Takeaway

> ðŸ”¥ Poisoning attacks exploit the openness of ML training pipelines â€” especially in **federated systems**, where the server trusts local updates.  
> Defending against them requires **robust aggregation**, **anomaly detection**, and **backdoor detection** mechanisms.