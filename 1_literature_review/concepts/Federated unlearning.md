## ğŸ§  1. What is Federated Unlearning?

**Federated Unlearning (FU)** is the process of **removing the influence of specific data points, samples, or clients** from a federated learning model after training.

Itâ€™s like pressing a â€œğŸ§¹ forgetâ€ button in a collaborative learning environment â€” ensuring that the global model no longer retains any trace of that clientâ€™s data or contribution.

Formally:

> Given a trained global model wTw_TwTâ€‹ and a subset of clients or data that must be forgotten, federated unlearning aims to produce a new model wTâ€²w_T'wTâ€²â€‹ â‰ˆ model trained _as if_ the forgotten data had never been included.

---

## ğŸ”’ 2. Why Is Federated Unlearning Important?

### (a) **Privacy & Regulation**

- Data privacy laws like **GDPR (â€œRight to be Forgottenâ€)** and **CCPA** require that users can request their data to be deleted.
    
- In federated learning, data never leaves clients â€” but **its influence remains** in the model parameters.
    

### (b) **Security**

- If a client is found to be **malicious** (e.g., performed a backdoor attack), its influence should be erased.
    

### (c) **Efficiency**

- Retraining from scratch (with remaining clients) is too costly.
    
- FU seeks **efficient approximate unlearning** methods.
    

---

## âš™ï¸ 3. How Federated Unlearning Works (Conceptually)

Letâ€™s recall the standard **FL loop:**

1. Server sends global model wtw_twtâ€‹.
    
2. Clients train locally â†’ send updates Î”wtk\Delta w_t^kÎ”wtkâ€‹.
    
3. Server aggregates:
    
    wt+1=wt+âˆ‘kÎ±kÎ”wtkw_{t+1} = w_t + \sum_k \alpha_k \Delta w_t^kwt+1â€‹=wtâ€‹+kâˆ‘â€‹Î±kâ€‹Î”wtkâ€‹

Now suppose **client C** requests unlearning.  
The goal: remove the effect of **client Câ€™s updates** from wTw_TwTâ€‹.

### Two general strategies:

#### (a) **Exact Unlearning**

Retrain the model from scratch using only remaining clients.  
âœ… Perfect unlearning  
âŒ Very costly.

#### (b) **Approximate Unlearning**

Estimate and reverse the influence of the forgotten client using mathematical or algorithmic approximations.  
âœ… Efficient  
âŒ Small residual influence may remain.

---

## ğŸ§© 4. Types of Federated Unlearning

|Type|Description|Example|
|---|---|---|
|**Client-Level Unlearning**|Remove all contributions from one or more clients.|When a user leaves the federation or withdraws consent.|
|**Sample-Level Unlearning**|Remove specific data samples from a client.|A patient revokes consent for their scan.|
|**Class-Level Unlearning**|Forget a specific label/class.|Forget all â€œcatâ€ images from the dataset.|
|**Task-Level Unlearning**|Forget an entire task in multi-task FL.|A hospital decides to stop contributing disease X data.|

---

## ğŸ§® 5. Mathematical Perspective

Suppose after TTT rounds, the global model is:

wT=w0+âˆ‘t=1Tâˆ‘k=1KÎ±ktÎ”wtkw_T = w_0 + \sum_{t=1}^{T} \sum_{k=1}^{K} \alpha_k^t \Delta w_t^kwTâ€‹=w0â€‹+t=1âˆ‘Tâ€‹k=1âˆ‘Kâ€‹Î±ktâ€‹Î”wtkâ€‹

If client jjj must be forgotten, approximate unlearning computes:

wTâ€²=wTâˆ’âˆ‘t=1TÎ±jtÎ”wtjw_T' = w_T - \sum_{t=1}^{T} \alpha_j^t \Delta w_t^jwTâ€²â€‹=wTâ€‹âˆ’t=1âˆ‘Tâ€‹Î±jtâ€‹Î”wtjâ€‹

That is, **subtract the cumulative contribution** of the forgotten client from the final model.

However, because of nonlinearities and interactions between clientsâ€™ updates, this is an approximation â€” not exact.

---

## ğŸ§° 6. Common Federated Unlearning Methods

### **(1) Server-Side Unlearning**

The server maintains logs of all past client updates.  
When unlearning is needed, it re-aggregates the model **excluding** the undesired clientsâ€™ updates.

- âœ… Simple to implement.
    
- âŒ Requires storing all update history (high memory).
    

### **(2) Knowledge Distillation Unlearning**

A new student model is trained using outputs of the old model, but **without the data of forgotten clients**.

- Retains most knowledge.
    
- Ensures data removal indirectly.
    

### **(3) Gradient Reversal Unlearning**

Estimate the gradient contribution of the target client and apply the **negative** gradient to cancel its effect.

### **(4) Mask-based or Pruning-based Unlearning**

Remove or adjust neurons associated with the forgotten clientâ€™s updates.

### **(5) Reconstruction-based Unlearning**

Train a small correction model that counteracts the forgotten dataâ€™s influence on the global model.

---

## ğŸ” 7. Evaluation Metrics

|Metric|Description|
|---|---|
|**Unlearning Effectiveness**|How well the forgotten dataâ€™s influence is removed.|
|**Model Utility**|How much performance is retained on remaining data.|
|**Efficiency**|Computation and communication cost vs. retraining.|
|**Privacy Guarantee**|Whether unlearning satisfies formal privacy laws (e.g., Îµ-differential unlearning).|

---

## âš–ï¸ 8. Comparison: Retraining vs Federated Unlearning

|Aspect|Retraining|Federated Unlearning|
|---|---|---|
|**Accuracy**|Exact|Approximate|
|**Cost**|Very high|Low to moderate|
|**Data Needed**|All clientsâ€™ data again|Only existing model updates|
|**Scalability**|Poor|High|
|**Compliance**|Full|Partial (depends on guarantee)|

---

## ğŸ” 9. Example Scenario

Imagine a **healthcare federated system** across hospitals:

- 10 hospitals collaboratively train a diagnostic model.
    
- Later, **Hospital #4** discovers labeling errors or withdraws participation.
    
- Instead of retraining the model from the remaining 9 hospitals, the central server performs **federated unlearning** to remove Hospital #4â€™s contribution efficiently.
    

---

## ğŸ§  10. Research & Frameworks

|Paper / Framework|Contribution|
|---|---|
|**â€œFederated Unlearning: Enabling Efficient Data Deletion in Federated Learningâ€ (NeurIPS 2021)**|Introduced first formal framework for FU.|
|**Shen et al., 2022 â€“ "FedEraser"**|Efficient unlearning via update tracking and reconstruction.|
|**Liu et al., 2023 â€“ "ForgetFL"**|Distillation-based unlearning method.|
|**Zhang et al., 2024 â€“ "FedUnlearn++"**|Adaptive unlearning for multiple forgetting requests.|

---

## ğŸš§ 11. Challenges in Federated Unlearning

|Challenge|Description|
|---|---|
|**Non-IID Data**|Clientsâ€™ data vary, making influence estimation hard.|
|**Interaction Effects**|Clientsâ€™ updates interact nonlinearly.|
|**Scalability**|Unlearning many clients efficiently is complex.|
|**Verification**|How to _prove_ that unlearning is successful.|
|**Security Risks**|Adversaries could misuse unlearning to manipulate models.|

---

## ğŸ§© 12. Summary Table

|Aspect|Description|
|---|---|
|**Goal**|Remove influence of certain clients or data from global model|
|**Motivation**|Privacy compliance, security, or user consent withdrawal|
|**Approaches**|Exact retraining or approximate unlearning|
|**Key Metrics**|Unlearning success, model utility, efficiency|
|**Challenges**|Verification, scalability, data heterogeneity|

---

## ğŸ§© 13. Example (Conceptual)

Letâ€™s visualize:

`Round 1: Client A, B, C â†’ contribute updates Round 2: Global Model Aggregates Round 3: Client B requests to be forgotten  â†’ Server removes Client Bâ€™s past updates from model â†’ Re-aggregates using A, Câ€™s contributions â†’ New model behaves as if B never participated`